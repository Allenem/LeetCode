{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Simple RNN with Backpropagation Through Time (BPTT)\n",
    "\n",
    "## Task: Implement a Simple RNN with Backpropagation Through Time (BPTT)\n",
    "\n",
    "Your task is to implement a simple Recurrent Neural Network (RNN) and backpropagation through time (BPTT) to learn from sequential data. The RNN will process input sequences, update hidden states, and perform backpropagation to adjust weights based on the error gradient.\n",
    "\n",
    "Write a class `SimpleRNN` with the following methods:\n",
    "\n",
    "- `__init__(self, input_size, hidden_size, output_size)`: Initializes the RNN with random weights and zero biases.\n",
    "- `forward(self, x)`: Processes a sequence of inputs and returns the hidden states and output.\n",
    "- `backward(self, x, y, learning_rate)`: Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n",
    "\n",
    "In this task, the RNN will be trained on sequence prediction, where the network will learn to predict the next item in a sequence. You should use MSE as the loss function.\n",
    "\n",
    "Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define sequence and labels\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "\n",
    "# Forward pass\n",
    "output = rnn.forward(input_sequence)\n",
    "\n",
    "# Backward pass\n",
    "rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# The output should show the RNN predictions for each step of the input sequence.\n",
    "```\n",
    "  \n",
    "## Understanding Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous inputs. They are particularly useful for tasks where context or sequential order is important, such as language modeling, time series forecasting, and sequence prediction.\n",
    "\n",
    "## RNN Architecture\n",
    "\n",
    "An RNN processes inputs one at a time while maintaining a hidden state that gets updated at each time step. The core equations governing the forward pass of an RNN are:\n",
    "\n",
    "**Hidden State Update:**\n",
    " \n",
    "$$h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$$\n",
    " \n",
    "**Output Computation:**\n",
    " \n",
    "$$y_t = W_{hy}h_t + b_y$$\n",
    " \n",
    "Where:\n",
    "\n",
    "- $x_t$ is the input at time step $t$.\n",
    "- $h_t$ is the hidden state at time step $t$.\n",
    "- $W_{xh}$ is the weight matrix for input to hidden state.\n",
    "- $W_{hh}$ is the weight matrix for hidden state to hidden state.\n",
    "- $W_{hy}$ is the weight matrix for hidden state to output.\n",
    "- $b_h$ and $b_y$ are the bias terms for the hidden state and output, respectively.\n",
    "- $\\tanh$ is the hyperbolic tangent activation function applied element-wise.\n",
    "\n",
    "## Forward Pass Implementation\n",
    "\n",
    "In the forward pass, we iterate over each element in the input sequence, updating the hidden state and computing the output:\n",
    "\n",
    "1. Initialize the hidden state $h_0$ to zeros.\n",
    "\n",
    "2. For each time step $t$:\n",
    "   - Compute $h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$.\n",
    "   - Compute $y_t = W_{hy}h_t + b_y$.\n",
    "   - Store $x_t$, $h_t$, and $y_t$ for use in backpropagation.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The loss function measures the discrepancy between the predicted outputs and the actual target values. For sequence prediction tasks, we often use the Mean Squared Error (MSE) loss:\n",
    "\n",
    "$$\\text{Loss} = \\frac{1}{T} \\sum_{t=1}^{T} (\\hat{y}_t - y_t)^2$$ \n",
    " \n",
    "Where $T$ is the length of the sequence, $\\hat{y}_t$ is the predicted output, and $y_t$ is the actual target at time step $t$.\n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "BPTT is the process of training RNNs by unrolling them through time and applying backpropagation to compute gradients for each time step. The key steps in BPTT are:\n",
    "\n",
    "1. Compute the gradient of the loss with respect to the outputs:\n",
    "\n",
    "$$\\frac{d\\text{L}}{dy_t} = \\hat{y}_t - y_t$$\n",
    "\n",
    "2. Compute the gradients for the output layer weights and biases:\n",
    " \n",
    "$$dW_{hy} += \\frac{d\\text{L}}{dy_t} \\cdot h_t^T$$\n",
    "$$db_y += \\frac{d\\text{L}}{dy_t}$$\n",
    " \n",
    "3. Backpropagate the gradients through the hidden layers:\n",
    " \n",
    "$$dh_t = W_{hy}^T \\cdot \\frac{d\\text{L}}{dy_t} + dh_{t+1}$$\n",
    "$$dh_{raw} = dh_t \\circ (1 - h_t^2)$$\n",
    " \n",
    "Where $\\circ$ denotes element-wise multiplication, and $(1 - h_t^2)$ is the derivative of the $\\tanh$ activation function.\n",
    "\n",
    "4. Compute the gradients for the hidden layer weights and biases:\n",
    " \n",
    "$$dW_{xh} += dh_{raw} \\cdot x_t^T$$\n",
    "$$dW_{hh} += dh_{raw} \\cdot h_{t-1}^T$$\n",
    "$$db_h += dh_{raw}$$\n",
    "\n",
    "We repeat steps 1-4 for each time step in reverse order (from $T$ to 1), accumulating the gradients. The term $dh_{t+1}$ represents the gradient flowing from the next time step, initialized to zeros at the last time step.\n",
    "\n",
    "## Updating Weights\n",
    "\n",
    "After computing the gradients, we update the weights and biases using gradient descent:\n",
    "\n",
    "$$W_{xh} -= \\text{learning\\_rate} \\times dW_{xh}$$\n",
    "\n",
    "$$W_{hh} -= \\text{learning\\_rate} \\times dW_{hh}$$\n",
    "\n",
    "$$W_{hy} -= \\text{learning\\_rate} \\times dW_{hy}$$\n",
    "\n",
    "$$b_h -= \\text{learning\\_rate} \\times db_h$$\n",
    "\n",
    "$$b_y -= \\text{learning\\_rate} \\times db_y$$\n",
    " \n",
    "## Implementing the RNN\n",
    "\n",
    "To implement the RNN with BPTT, follow these steps:\n",
    "\n",
    "1. **Initialization**: Initialize the weight matrices $W_{xh}$, $W_{hh}$, and $W_{hy}$ with small random values and biases $b_h$ and $b_y$ with zeros.\n",
    "2. **Forward Pass**: Implement the forward method to process the input sequence, updating the hidden states and computing the outputs at each time step. Store the inputs, hidden states, and outputs for use in backpropagation.\n",
    "3. **Backward Pass**: Implement the backward method to perform BPTT. Compute the gradients at each time step in reverse order, accumulate them, and update the weights and biases.\n",
    "4. **Training Loop**: Train the RNN over multiple epochs by repeatedly performing forward and backward passes and updating the weights.\n",
    "\n",
    "## Tips for Implementation\n",
    "\n",
    "- **Gradient Clipping**: To prevent exploding gradients, consider applying gradient clipping, which scales down gradients if they exceed a certain threshold.\n",
    "- **Learning Rate**: Choose an appropriate learning rate. If the learning rate is too high, the training may become unstable.\n",
    "- **Debugging**: Check the dimensions of all matrices and vectors to ensure they align correctly during matrix multiplication.\n",
    "- **Testing**: Start with small sequences and hidden sizes to test your implementation before scaling up.\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "Suppose we have an input sequence $x = [x_1, x_2]$ and target sequence $y = [y_1, y_2]$. Here's how you would compute the forward and backward passes:\n",
    "\n",
    "1. Forward Pass:\n",
    "\n",
    "   - At $t=1$:\n",
    "     - Compute $h_1 = \\tanh(W_{xh}x_1 + W_{hh}h_0 + b_h)$.\n",
    "     - Compute $y_1 = W_{hy}h_1 + b_y$.\n",
    "   - At $t=2$:\n",
    "     - Compute $h_2 = \\tanh(W_{xh}x_2 + W_{hh}h_1 + b_h)$.\n",
    "     - Compute $y_2 = W_{hy}h_2 + b_y$.\n",
    "     \n",
    "2. Compute Loss: $\\text{Loss} = \\frac{1}{2} \\sum_{t=1}^{2} (\\hat{y}_t - y_t)^2$.\n",
    "3. Backward Pass: Starting from $t=2$ to $t=1$:\n",
    "   \n",
    "   - At $t=2$:\n",
    "     - Compute $d\\text{L}/d\\hat{y}_2 = \\hat{y}_2 - y_2$.\n",
    "     - Compute gradients $dW_{hy}$, $db_y$, $dh_2$.\n",
    "   - At $t=1$:\n",
    "     - Compute $d\\text{L}/d\\hat{y}_1 = \\hat{y}_1 - y_1$.\n",
    "     - Compute gradients $dW_{hy}$, $db_y$, $dh_1$, including the $dh_2$ term from the next time step.\n",
    "     \n",
    "4. Update Weights: Adjust the weights and biases using the accumulated gradients.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By understanding the forward and backward passes in RNNs and how to compute and update gradients through BPTT, you can implement an RNN from scratch. This foundational knowledge is crucial for working with more advanced neural network architectures and understanding deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n",
    "        outputs = []\n",
    "        self.last_inputs = []\n",
    "        self.last_hiddens = [h]\n",
    "        \n",
    "        for t in range(len(x)):\n",
    "            self.last_inputs.append(x[t].reshape(-1, 1))\n",
    "            h = np.tanh(np.dot(self.W_xh, self.last_inputs[t]) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            y = np.dot(self.W_hy, h) + self.b_y\n",
    "            outputs.append(y)\n",
    "            self.last_hiddens.append(h)\n",
    "        \n",
    "        self.last_outputs = outputs\n",
    "        return np.array(outputs)\n",
    "\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for t in reversed(range(len(x))):\n",
    "            dy = self.last_outputs[t] - y[t].reshape(-1, 1)  # (Predicted - Actual)\n",
    "            dW_hy += np.dot(dy, self.last_hiddens[t+1].T)\n",
    "            db_y += dy\n",
    "\n",
    "            dh = np.dot(self.W_hy.T, dy) + dh_next\n",
    "            dh_raw = (1 - self.last_hiddens[t+1] ** 2) * dh  # Derivative of tanh\n",
    "\n",
    "            dW_xh += np.dot(dh_raw, self.last_inputs[t].T)\n",
    "            dW_hh += np.dot(dh_raw, self.last_hiddens[t].T)\n",
    "            db_h += dh_raw\n",
    "\n",
    "            dh_next = np.dot(self.W_hh.T, dh_raw)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W_xh -= learning_rate * dW_xh\n",
    "        self.W_hh -= learning_rate * dW_hh\n",
    "        self.W_hy -= learning_rate * dW_hy\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "np.random.seed(42)\n",
      "input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
      "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
      "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
      "# Train the RNN over multiple epochs\n",
      "for epoch in range(100):\n",
      "    output = rnn.forward(input_sequence)\n",
      "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[[2.24143915]]\n",
      "\n",
      " [[3.18450265]]\n",
      "\n",
      " [[4.04305928]]\n",
      "\n",
      " [[4.57419398]]]\n",
      "\n",
      "Expected:\n",
      "[[[2.24143915]],[[3.18450265]],[[4.04305928]],[[4.57419398]]]\n",
      "\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "np.random.seed(42)\n",
      "input_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\n",
      "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
      "rnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n",
      "# Train the RNN over multiple epochs\n",
      "for epoch in range(100):\n",
      "    output = rnn.forward(input_sequence)\n",
      "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[[2.42201379]]\n",
      "\n",
      " [[3.44167595]]\n",
      "\n",
      " [[3.6129965 ]]\n",
      "\n",
      " [[4.50660152]]]\n",
      "\n",
      "Expected:\n",
      "[[[2.42201379]],[[3.44167595]],[[3.6129965 ]],[[4.50660152]]]\n",
      "\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "np.random.seed(42)\n",
      "input_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\n",
      "expected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\n",
      "rnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n",
      "# Train the RNN over multiple epochs\n",
      "for epoch in range(50):\n",
      "    output = rnn.forward(input_sequence)\n",
      "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[[3.28424506]\n",
      "  [5.93532247]]\n",
      "\n",
      " [[3.60393582]\n",
      "  [6.82013468]]\n",
      "\n",
      " [[3.52586543]\n",
      "  [6.58278163]]\n",
      "\n",
      " [[3.61336207]\n",
      "  [6.84916339]]]\n",
      "\n",
      "Expected:\n",
      "[[[3.28424506],[5.93532247]],[[3.60393582],[6.82013468]],[[3.52586543],[6.58278163]],[[3.61336207],[6.84916339]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
    "rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "# Train the RNN over multiple epochs\n",
    "for epoch in range(100):\n",
    "    output = rnn.forward(input_sequence)\n",
    "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "print('Test Case 1: Accepted') if np.allclose(output, np.array([[[2.24143915]],[[3.18450265]],[[4.04305928]],[[4.57419398]]])) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nnp.random.seed(42)\\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\\n# Train the RNN over multiple epochs\\nfor epoch in range(100):\\n    output = rnn.forward(input_sequence)\\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[[2.24143915]],[[3.18450265]],[[4.04305928]],[[4.57419398]]]')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "input_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\n",
    "expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n",
    "rnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n",
    "# Train the RNN over multiple epochs\n",
    "for epoch in range(100):\n",
    "    output = rnn.forward(input_sequence)\n",
    "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "print('Test Case 2: Accepted') if np.allclose(output, np.array([[[2.42201379]],[[3.44167595]],[[3.6129965 ]],[[4.50660152]]])) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nnp.random.seed(42)\\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\\n# Train the RNN over multiple epochs\\nfor epoch in range(100):\\n    output = rnn.forward(input_sequence)\\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[[2.42201379]],[[3.44167595]],[[3.6129965 ]],[[4.50660152]]]')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "input_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\n",
    "expected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\n",
    "rnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n",
    "# Train the RNN over multiple epochs\n",
    "for epoch in range(50):\n",
    "    output = rnn.forward(input_sequence)\n",
    "    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n",
    "print('Test Case 3: Accepted') if np.allclose(output, np.array([[[3.28424506],[5.93532247]],[[3.60393582],[6.82013468]],[[3.52586543],[6.58278163]],[[3.61336207],[6.84916339]]])) else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nnp.random.seed(42)\\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\\n# Train the RNN over multiple epochs\\nfor epoch in range(50):\\n    output = rnn.forward(input_sequence)\\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[[3.28424506],[5.93532247]],[[3.60393582],[6.82013468]],[[3.52586543],[6.58278163]],[[3.61336207],[6.84916339]]]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
