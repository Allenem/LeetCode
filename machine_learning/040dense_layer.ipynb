{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Custom Dense Layer in Python\n",
    "\n",
    "You are provided with a base `Layer` class that defines the structure of a neural network layer. Your task is to implement a subclass called `Dense`, which represents a fully connected neural network layer. The `Dense` class should extend the `Layer` class and implement the following methods:\n",
    "\n",
    "1. Initialization (`__init__`):\n",
    "    - Define the layer with a specified number of neurons (`n_units`) and an optional input shape (`input_shape`).\n",
    "    - Set up placeholders for the layer's weights (`W`), biases (`w0`), and optimizers.\n",
    "2. Weight Initialization (`initialize`):\n",
    "    - Initialize the weights `W` using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`, and bias `w0` should be set to zero\n",
    "    - Initialize optimizers for `W` and `w0`.\n",
    "3. Parameter Count (`parameters`):\n",
    "    - Return the total number of trainable parameters in the layer, which includes the parameters in `W` and `w0`.\n",
    "4. Forward Pass (`forward_pass`):\n",
    "    - Compute the output of the layer by performing a dot product between the input `X` and the weight matrix `W`, and then adding the bias `w0`.\n",
    "5. Backward Pass (`backward_pass`):\n",
    "    - Calculate and return the gradient with respect to the input.\n",
    "    - If the layer is trainable, update the weights and biases using the optimizer's update rule.\n",
    "6. Output Shape (`output_shape`):\n",
    "    - Return the shape of the output produced by the forward pass, which should be `(self.n_units,)`.\n",
    "\n",
    "Objective: Extend the `Layer` class by implementing the `Dense` class to ensure it functions correctly within a neural network framework.\n",
    "\n",
    "Example Usage:\n",
    "```python\n",
    "# Initialize a Dense layer with 3 neurons and input shape (2,)\n",
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "\n",
    "# Define a mock optimizer with a simple update rule\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad\n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "\n",
    "# Initialize the Dense layer with the mock optimizer\n",
    "dense_layer.initialize(optimizer)\n",
    "\n",
    "# Perform a forward pass with sample input data\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "print(\"Forward pass output:\", output)\n",
    "\n",
    "# Perform a backward pass with sample gradient\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "print(\"Backward pass output:\", back_output)\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Forward pass output: [[-0.00655782  0.01429615  0.00905812]]\n",
    "Backward pass output: [[ 0.00129588  0.00953634]]\n",
    "``` \n",
    "  \n",
    "## Understanding the Dense Layer\n",
    "\n",
    "The Dense layer, also known as a fully connected layer, is a fundamental building block in neural networks. It connects each input neuron to each output neuron, hence the term \"fully connected.\"\n",
    "\n",
    "1. Weight Initialization\n",
    "\n",
    "In the `initialize` method, weights are typically initialized using a uniform distribution within a certain range. For a Dense layer, a common practice is to set this range as:\n",
    "\n",
    "$$extlimit = \\frac{1}{\\sqrt{extinput_{shape}[0]}}$$\n",
    " \n",
    "This initialization helps in maintaining a balance in the distribution of weights, preventing issues like vanishing or exploding gradients during training.\n",
    "\n",
    "2. Forward Pass\n",
    "\n",
    "During the forward pass, the input data $X$ is multiplied by the weight matrix $W$ and added to the bias $w0$ to produce the output:\n",
    "\n",
    "$$extoutput = X \\cdot W + w0$$\n",
    "\n",
    "3. Backward Pass\n",
    "\n",
    "The backward pass computes the gradients of the loss function with respect to the input data, weight, and bias. If the layer is trainable, it updates the weights and biases using the optimizer's update rule:\n",
    "\n",
    "$$W = W - \\eta \\cdot grad_W$$\n",
    "$$w0 = w0 - \\eta \\cdot grad_{w0}$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $grad_W$ and $grad_{w0}$ are the gradients of the weights and biases, respectively.\n",
    "\n",
    "4. Output Shape\n",
    "\n",
    "The shape of the output from a Dense layer is determined by the number of neurons in the layer. If a layer has `n_units` neurons, the output shape will be `(n_units,)`.\n",
    "\n",
    "Resources:\n",
    "\n",
    "[CS231n: Fully Connected Layer](https://cs231n.github.io/neural-networks-2/#fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# DO NOT CHANGE SEED\n",
    "np.random.seed(42)\n",
    "\n",
    "# DO NOT CHANGE LAYER CLASS\n",
    "class Layer(object):\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "\n",
    "    def layer_name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def parameters(self):\n",
    "        return 0\n",
    "\n",
    "    def forward_pass(self, X, training):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Your task is to implement the Dense class based on the above structure\n",
    "class Dense(Layer):\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "        self.w0 = np.zeros((1, self.n_units))\n",
    "        self.W_opt  = copy.copy(optimizer)\n",
    "        self.w0_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.w0\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        W = self.W\n",
    "        if self.trainable:\n",
    "            grad_w = self.layer_input.T.dot(accum_grad)\n",
    "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "            self.W = self.W_opt.update(self.W, grad_w)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "        accum_grad = accum_grad.dot(W.T)\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        return (self.n_units, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
      "\n",
      "class MockOptimizer:\n",
      "    def update(self, weights, grad):\n",
      "        return weights - 0.01 * grad\n",
      "\n",
      "optimizer = MockOptimizer()\n",
      "dense_layer.initialize(optimizer)\n",
      "\n",
      "X = np.array([[1, 2]])\n",
      "output = dense_layer.forward_pass(X)\n",
      "\n",
      "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
      "back_output = dense_layer.backward_pass(accum_grad)\n",
      "print(back_output)\n",
      "\n",
      "Output:\n",
      "[[ 0.20816524 -0.22928937]]\n",
      "\n",
      "Expected:\n",
      "[[ 0.20816524, -0.22928937]]\n"
     ]
    }
   ],
   "source": [
    "dense_layer = Dense(n_units=3, input_shape=(2,))\n",
    "\n",
    "class MockOptimizer:\n",
    "    def update(self, weights, grad):\n",
    "        return weights - 0.01 * grad\n",
    "\n",
    "optimizer = MockOptimizer()\n",
    "dense_layer.initialize(optimizer)\n",
    "\n",
    "X = np.array([[1, 2]])\n",
    "output = dense_layer.forward_pass(X)\n",
    "\n",
    "accum_grad = np.array([[0.1, 0.2, 0.3]])\n",
    "back_output = dense_layer.backward_pass(accum_grad)\n",
    "\n",
    "print('Test Case 1: Accepted') if np.allclose(back_output, [[ 0.20816524, -0.22928937]]) else print('Test Case 1: Rejected')\n",
    "print('Input:')\n",
    "print('dense_layer = Dense(n_units=3, input_shape=(2,))\\n\\nclass MockOptimizer:\\n    def update(self, weights, grad):\\n        return weights - 0.01 * grad\\n\\noptimizer = MockOptimizer()\\ndense_layer.initialize(optimizer)\\n\\nX = np.array([[1, 2]])\\noutput = dense_layer.forward_pass(X)\\n\\naccum_grad = np.array([[0.1, 0.2, 0.3]])\\nback_output = dense_layer.backward_pass(accum_grad)\\nprint(back_output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(back_output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[ 0.20816524, -0.22928937]]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
