{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Accuracy Score\n",
    "\n",
    "Write a Python function to calculate the accuracy score of a model's predictions. The function should take in two 1D numpy arrays: y_true, which contains the true labels, and y_pred, which contains the predicted labels. It should return the accuracy score as a float.\n",
    "\n",
    "Example:\n",
    "```py\n",
    "    y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "    y_pred = np.array([1, 0, 0, 1, 0, 1])\n",
    "    output = accuracy_score(y_true, y_pred)\n",
    "    print(output)\n",
    "    # Output:\n",
    "    # 0.8333333333333334\n",
    "    \n",
    "    Reasoning:\n",
    "    The function compares the true labels with the predicted labels and calculates the ratio of correct predictions to the total number of predictions. In this example, there are 5 correct predictions out of 6, resulting in an accuracy score of 0.8333333333333334.\n",
    "```    \n",
    "  \n",
    "## Understanding Accuracy Score\n",
    "\n",
    "Accuracy is a metric used to evaluate the performance of a classification model. It is defined as the ratio of the number of correct predictions to the total number of predictions made. Mathematically, accuracy is given by:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    " \n",
    "In this problem, you will write a function to calculate the accuracy score given the true labels and the predicted labels. The function will compare the two arrays and compute the accuracy as the proportion of matching elements.\n",
    "\n",
    "Accuracy is a straightforward and commonly used metric for classification tasks. It provides a quick way to understand how well a model is performing, but it may not always be the best metric, especially for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    eq = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        eq += 1 if yt == yp else 0\n",
    "    return float(eq/len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "print(accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])))\n",
      "\n",
      "Output:\n",
      "0.8333333333333334\n",
      "\n",
      "Expected:\n",
      "0.8333333333333334\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "print(accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])))\n",
      "\n",
      "Output:\n",
      "0.5\n",
      "\n",
      "Expected:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print('Test Case 1: Accepted') if accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333333333333334 else print('Test Case 1: Rejected')\n",
    "print('Input:')\n",
    "print('print(accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])))\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.8333333333333334')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Test Case 2: Accepted') if accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5 else print('Test Case 2: Rejected')\n",
    "print('Input:')\n",
    "print('print(accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])))\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
