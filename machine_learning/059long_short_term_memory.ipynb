{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Long Short-Term Memory (LSTM) Network\n",
    "\n",
    "## Task: Implement Long Short-Term Memory (LSTM) Network\n",
    "\n",
    "Your task is to implement an LSTM network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs.\n",
    "\n",
    "Write a class `LSTM` with the following methods:\n",
    "\n",
    "- `__init__(self, input_size, hidden_size)`: Initializes the LSTM with random weights and zero biases.\n",
    "- `forward(self, x, initial_hidden_state, initial_cell_state)`: Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n",
    "\n",
    "The LSTM should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0]])\n",
    "initial_hidden_state = np.zeros((1, 1))\n",
    "initial_cell_state = np.zeros((1, 1))\n",
    "\n",
    "lstm = LSTM(input_size=1, hidden_size=1)\n",
    "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "\n",
    "print(final_h)\n",
    "\n",
    "# Expected Output:\n",
    "# [[0.73698596]] (approximate)\n",
    "```\n",
    "\n",
    "## Understanding Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "Long Short-Term Memory Networks are a special type of RNN designed to capture long-term dependencies in sequential data by using a more complex hidden state structure.\n",
    "\n",
    "## LSTM Gates and Their Functions\n",
    "\n",
    "For each time step $t$, the LSTM updates its cell state $c_t$ and hidden state $h_t$ using the current input $x_t$, the previous cell state $c_{t-1}$, and the previous hidden state $h_{t-1}$. The LSTM architecture consists of several gates that control the flow of information:\n",
    "\n",
    "- **Forget Gate ($f_t$)**: This gate decides what information to discard from the cell state. It looks at the previous hidden state $h_{t-1}$ and the current input $x_t$, and outputs a number between 0 and 1 for each number in the cell state. A 1 represents \"keep this\" while a 0 represents \"forget this\".\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$ \n",
    "\n",
    "- **Input Gate ($i_t$)**: This gate decides which new information will be stored in the cell state. It consists of two parts:\n",
    "    - A sigmoid layer that decides which values we'll update.\n",
    "    - A tanh layer that creates a vector of new candidate values $\\tilde{c}_t$ that could be added to the state.\n",
    "    \n",
    "    $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "    $$\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$ \n",
    "\n",
    "- **Cell State Update ($c_t$)**: This step updates the old cell state ($c_{t-1}$) into the new cell state ($c_t$). It multiplies the old state by the forget gate output, then adds the product of the input gate and the new candidate values.\n",
    " \n",
    "$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t$$\n",
    "\n",
    "- **Output Gate ($o_t$)**: This gate decides what parts of the cell state we're going to output. It uses a sigmoid function to determine which parts of the cell state to output, and then multiplies it by a tanh of the cell state to get the final output.\n",
    " \n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\cdot \\tanh(c_t)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- (W_f, W_i, W_c, W_o) are weight matrices for the forget gate, input gate, cell state, and output gate respectively.\n",
    "- (b_f, b_i, b_c, b_o) are bias vectors.\n",
    "- $\\sigma$ denotes the sigmoid activation function.\n",
    "- $\\cdot$ denotes element-wise multiplication.\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. Initialization: Start with the initial cell state (c_0) and hidden state (h_0).\n",
    "2. Sequence Processing: For each input (x_t) in the sequence:\n",
    "    - Compute forget gate (f_t), input gate (i_t), candidate cell state ($\\tilde{c}_t$), and output gate (o_t).\n",
    "    - Update cell state (c_t) and hidden state (h_t).\n",
    "3. Final Output: After processing all inputs, the final hidden state (h_T) (where (T) is the length of the sequence) contains information from the entire sequence.\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "Given:\n",
    "\n",
    "- Inputs: (x_1 = 1.0), (x_2 = 2.0), (x_3 = 3.0)\n",
    "- Initial states: (c_0 = 0.0), (h_0 = 0.0)\n",
    "- Simplified weights (for demonstration): (W_f = W_i = W_c = W_o = 0.5)\n",
    "- All biases: (b_f = b_i = b_c = b_o = 0.1)\n",
    "\n",
    "Compute:\n",
    "\n",
    "First time step ((t = 1)):\n",
    "\n",
    "$$\n",
    "f_1 = \\sigma(0.5 \\times [0.0, 1.0] + 0.1) = 0.6487 \\\\\n",
    "i_1 = \\sigma(0.5 \\times [0.0, 1.0] + 0.1) = 0.6487 \\\\\n",
    "\\tilde{c}_1 = \\tanh(0.5 \\times 1.0 + 0.1) = 0.5370 \\\\\n",
    "c_1 = f_1 \\times 0.0 + i_1 \\times \\tilde{c}_1 = 0.6487 \\times 0.0 + 0.6487 \\times 0.5370 = 0.3484 \\\\\n",
    "o_1 = \\sigma(0.5 \\times 1.0 + 0.1) = 0.6487 \\\\\n",
    "h_1 = o_1 \\times \\tanh(c_1) = 0.6487 \\times 0.3484 = 0.2169\n",
    "$$ \n",
    "\n",
    "Second time step ((t = 2)): (Calculations omitted for brevity, but follow the same pattern using (x_2 = 2.0) and the previous states)\n",
    "\n",
    "Third time step ((t = 3)): (Calculations omitted for brevity, but follow the same pattern using (x_3 = 3.0) and the previous states)\n",
    "\n",
    "The final hidden state (h_3) would be the result after these calculations.\n",
    "\n",
    "## Applications\n",
    "\n",
    "LSTMs are extensively used in various sequence modeling tasks, including machine translation, speech recognition, and time series forecasting, where capturing long-term dependencies is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x, initial_hidden_state, initial_cell_state):\n",
    "        \"\"\"\n",
    "        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n",
    "        \"\"\"\n",
    "        h = initial_hidden_state\n",
    "        c = initial_cell_state\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(len(x)):\n",
    "            xt = x[t].reshape(-1, 1)\n",
    "            concat = np.vstack((h, xt))\n",
    "            \n",
    "            # Forget Gate\n",
    "            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "            \n",
    "            # Input Gate\n",
    "            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "            \n",
    "            # Cell State Update\n",
    "            c = ft * c + it * c_tilde\n",
    "            \n",
    "            # Output Gate\n",
    "            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "            \n",
    "            # Hidden State\n",
    "            h = ot * np.tanh(c)\n",
    "            \n",
    "            outputs.append(h)\n",
    "            \n",
    "        return np.array(outputs), h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "input_sequence = np.array([[1.0], [2.0], [3.0]])\n",
      "initial_hidden_state = np.zeros((1, 1))\n",
      "initial_cell_state = np.zeros((1, 1))\n",
      "lstm = LSTM(input_size=1, hidden_size=1)\n",
      "# Set weights and biases for reproducibility\n",
      "lstm.Wf = np.array([[0.5, 0.5]])\n",
      "lstm.Wi = np.array([[0.5, 0.5]])\n",
      "lstm.Wc = np.array([[0.3, 0.3]])\n",
      "lstm.Wo = np.array([[0.5, 0.5]])\n",
      "lstm.bf = np.array([[0.1]])\n",
      "lstm.bi = np.array([[0.1]])\n",
      "lstm.bc = np.array([[0.1]])\n",
      "lstm.bo = np.array([[0.1]])\n",
      "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
      "print(final_h)\n",
      "\n",
      "Output:\n",
      "[[0.73698596]]\n",
      "\n",
      "Expected:\n",
      "[[0.73698596]]\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "input_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
      "initial_hidden_state = np.zeros((2, 1))\n",
      "initial_cell_state = np.zeros((2, 1))\n",
      "lstm = LSTM(input_size=2, hidden_size=2)\n",
      "# Set weights and biases for reproducibility\n",
      "lstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
      "lstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
      "lstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
      "lstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
      "lstm.bf = np.array([[0.1], [0.2]])\n",
      "lstm.bi = np.array([[0.1], [0.2]])\n",
      "lstm.bc = np.array([[0.1], [0.2]])\n",
      "lstm.bo = np.array([[0.1], [0.2]])\n",
      "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
      "print(final_h)\n",
      "\n",
      "Output:\n",
      "[[0.16613133]\n",
      " [0.40299449]]\n",
      "\n",
      "Expected:\n",
      "[[0.16613133], [0.40299449]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_sequence = np.array([[1.0], [2.0], [3.0]])\n",
    "initial_hidden_state = np.zeros((1, 1))\n",
    "initial_cell_state = np.zeros((1, 1))\n",
    "lstm = LSTM(input_size=1, hidden_size=1)\n",
    "# Set weights and biases for reproducibility\n",
    "lstm.Wf = np.array([[0.5, 0.5]])\n",
    "lstm.Wi = np.array([[0.5, 0.5]])\n",
    "lstm.Wc = np.array([[0.3, 0.3]])\n",
    "lstm.Wo = np.array([[0.5, 0.5]])\n",
    "lstm.bf = np.array([[0.1]])\n",
    "lstm.bi = np.array([[0.1]])\n",
    "lstm.bc = np.array([[0.1]])\n",
    "lstm.bo = np.array([[0.1]])\n",
    "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "print('Test Case 1: Accepted') if np.allclose(final_h, np.array([[0.73698596]])) else print('Test Case 1: Error')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ninput_sequence = np.array([[1.0], [2.0], [3.0]])\\ninitial_hidden_state = np.zeros((1, 1))\\ninitial_cell_state = np.zeros((1, 1))\\nlstm = LSTM(input_size=1, hidden_size=1)\\n# Set weights and biases for reproducibility\\nlstm.Wf = np.array([[0.5, 0.5]])\\nlstm.Wi = np.array([[0.5, 0.5]])\\nlstm.Wc = np.array([[0.3, 0.3]])\\nlstm.Wo = np.array([[0.5, 0.5]])\\nlstm.bf = np.array([[0.1]])\\nlstm.bi = np.array([[0.1]])\\nlstm.bc = np.array([[0.1]])\\nlstm.bo = np.array([[0.1]])\\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\\nprint(final_h)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(final_h)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[0.73698596]]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "input_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "initial_hidden_state = np.zeros((2, 1))\n",
    "initial_cell_state = np.zeros((2, 1))\n",
    "lstm = LSTM(input_size=2, hidden_size=2)\n",
    "# Set weights and biases for reproducibility\n",
    "lstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "lstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "lstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "lstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
    "lstm.bf = np.array([[0.1], [0.2]])\n",
    "lstm.bi = np.array([[0.1], [0.2]])\n",
    "lstm.bc = np.array([[0.1], [0.2]])\n",
    "lstm.bo = np.array([[0.1], [0.2]])\n",
    "outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n",
    "print('Test Case 2: Accepted') if np.allclose(final_h, np.array([[0.16613133], [0.40299449]])) else print('Test Case 2: Error')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\\ninitial_hidden_state = np.zeros((2, 1))\\ninitial_cell_state = np.zeros((2, 1))\\nlstm = LSTM(input_size=2, hidden_size=2)\\n# Set weights and biases for reproducibility\\nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\\nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\\nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\\nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\\nlstm.bf = np.array([[0.1], [0.2]])\\nlstm.bi = np.array([[0.1], [0.2]])\\nlstm.bc = np.array([[0.1], [0.2]])\\nlstm.bo = np.array([[0.1], [0.2]])\\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\\nprint(final_h)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(final_h)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[0.16613133], [0.40299449]]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
