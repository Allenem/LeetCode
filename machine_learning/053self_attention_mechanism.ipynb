{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Self-Attention Mechanism\n",
    "\n",
    "Task: Implement the Self-Attention Mechanism\n",
    "\n",
    "Your task is to implement the self-attention mechanism, which is a fundamental component of transformer models, widely used in natural language processing and computer vision tasks. The self-attention mechanism allows a model to dynamically focus on different parts of the input sequence when generating a contextualized representation.\n",
    "\n",
    "Your function should return the self-attention output as a numpy array.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Expected Output:\n",
    "# [[1.660477 2.660477]\n",
    "#  [2.339523 3.339523]]\n",
    "```\n",
    "\n",
    "## Self-Attention Mechanism\n",
    "\n",
    "This document provides an overview of the self-attention mechanism, which is fundamental in transformer models for tasks like natural language processing and computer vision.\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "- The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence dynamically. This ability to assign varying levels of importance is key to capturing long-range dependencies, which is highly effective in tasks like language translation, text summarization, and machine vision.\n",
    "- The self-attention operation calculates attention scores for every input, determining how much focus to put on other inputs when generating a contextualized representation.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "- Self-Attention Calculation:\n",
    "\n",
    "Given an input sequence $X$:\n",
    "\n",
    "$$Q = XW_q, K = XW_k, V = XW_v$$\n",
    "\n",
    "Where $Q$, $K$, and $V$ represent the Query, Key, and Value matrices respectively, and $W_q$, $W_k$, and $W_v$ are learned weight matrices.\n",
    "\n",
    "The attention score is computed as:\n",
    " \n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    " \n",
    "Where $d_k$ is the dimension of the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d = Q.shape[1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    attention_output = np.dot(attention_weights, V)\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 0], [0, 1]])\n",
      "W_q = np.array([[1, 0], [0, 1]])\n",
      "W_k = np.array([[1, 0], [0, 1]])\n",
      "W_v = np.array([[1, 2], [3, 4]])\n",
      "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
      "output = self_attention(Q, K, V)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n",
      "\n",
      "Expected:\n",
      "[[1.660477, 2.660477], [2.339523, 3.339523]]\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 1], [1, 0]])\n",
      "W_q = np.array([[1, 0], [0, 1]])\n",
      "W_k = np.array([[1, 0], [0, 1]])\n",
      "W_v = np.array([[1, 2], [3, 4]])\n",
      "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
      "output = self_attention(Q, K, V)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[3.00928465 4.6790462 ]\n",
      " [2.5        4.        ]]\n",
      "\n",
      "Expected:\n",
      "[[3.00928465, 4.6790462], [2.5, 4.0]]\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]])\n",
      "W_q = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
      "W_k = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
      "W_v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
      "output = self_attention(Q, K, V)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[[ 8.         10.         12.        ]\n",
      " [ 8.61987385 10.61987385 12.61987385]\n",
      " [ 7.38012615  9.38012615 11.38012615]]\n",
      "\n",
      "Expected:\n",
      "[[8.0, 10.0, 12.0], [8.61987385, 10.61987385, 12.61987385], [7.38012615, 9.38012615, 11.38012615]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "print('Test Case 1: Accepted') if np.allclose(output, np.array([[1.660477, 2.660477], [2.339523, 3.339523]])) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 0], [0, 1]])\\nW_q = np.array([[1, 0], [0, 1]])\\nW_k = np.array([[1, 0], [0, 1]])\\nW_v = np.array([[1, 2], [3, 4]])\\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\\noutput = self_attention(Q, K, V)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[1.660477, 2.660477], [2.339523, 3.339523]]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [1, 0]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "print('Test Case 2: Accepted') if np.allclose(output, np.array([[3.00928465, 4.6790462], [2.5, 4.0]])) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 1], [1, 0]])\\nW_q = np.array([[1, 0], [0, 1]])\\nW_k = np.array([[1, 0], [0, 1]])\\nW_v = np.array([[1, 2], [3, 4]])\\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\\noutput = self_attention(Q, K, V)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[3.00928465, 4.6790462], [2.5, 4.0]]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]])\n",
    "W_q = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
    "W_k = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
    "W_v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)\n",
    "print('Test Case 3: Accepted') if np.allclose(output, np.array([[8.0, 10.0, 12.0], [8.61987385, 10.61987385, 12.61987385], [7.38012615, 9.38012615, 11.38012615]])) else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]])\\nW_q = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\\nW_k = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]])\\nW_v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\\noutput = self_attention(Q, K, V)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[[8.0, 10.0, 12.0], [8.61987385, 10.61987385, 12.61987385], [7.38012615, 9.38012615, 11.38012615]]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
