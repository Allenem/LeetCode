{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement F-Score Calculation for Binary Classification\n",
    "\n",
    "## Task: Implement F-Score Calculation for Binary Classification\n",
    "\n",
    "Your task is to implement a function that calculates the F-Score for a binary classification task. The F-Score combines both Precision and Recall into a single metric, providing a balanced measure of a model's performance.\n",
    "\n",
    "Write a function `f_score(y_true, y_pred, beta)` where:\n",
    "\n",
    "- `y_true`: A numpy array of true labels (binary).\n",
    "- `y_pred`: A numpy array of predicted labels (binary).\n",
    "- `beta`: A float value that adjusts the importance of Precision and Recall. When beta=1, it computes the F1-Score, a balanced measure of both Precision and Recall.\n",
    "\n",
    "The function should return the F-Score rounded to three decimal places.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1])\n",
    "beta = 1\n",
    "\n",
    "print(f_score(y_true, y_pred, beta))\n",
    "\n",
    "# Expected Output:\n",
    "# 0.857\n",
    "```\n",
    "\n",
    "## Understanding F-Score in Classification\n",
    "\n",
    "F-Score, also called F-measure, is a measure of predictive performance that's calculated from the Precision and Recall metrics.\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "The $F_{\\beta}$ score applies additional weights, valuing one of precision or recall more than the other. When \\beta equals to 1, also known as F1-Score, it symmetrically represents both precision and recall in one metric. The F-Score can be calculated using the following formula:\n",
    "\n",
    "$$F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{(\\beta^2 \\cdot Precision) + Recall}$$\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "<!-- TP TN FP FN table center -->.\n",
    "\n",
    "<center>\n",
    "\n",
    "|  |  | Pred | Pred |\n",
    "| --- | --- | --- | --- |\n",
    "|  |  | Positive | Negative |\n",
    "| **True** | Positive | TP | FN |\n",
    "| **True** | Negative | FP | TN |\n",
    "\n",
    "</center>\n",
    "\n",
    "Where:\n",
    "\n",
    "- `Recall`: The number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "- `Precision`: The number of true positive results divided by the number of all samples predicted to be positive, including those not identified correctly.\n",
    "\n",
    "In this problem, you will implement a function to calculate the `F-Score` given the true labels, predicted labels and the Beta value of a binary classification task. The results should be rounded to three decimal places.\n",
    "\n",
    "If the denominator is zero, the F-Score should be 0.0 to avoid division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_score(y_true, y_pred, beta):\n",
    "    \"\"\"\n",
    "    Calculate F-Score for a binary classification task.\n",
    "\n",
    "    :param y_true: Numpy array of true labels\n",
    "    :param y_pred: Numpy array of predicted labels\n",
    "    :param beta: The weight of precision in the harmonic mean\n",
    "    :return: F-Score rounded to three decimal places\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt==yp==1: tp+=1\n",
    "        elif yt==yp==0: tn+=1\n",
    "        elif yt==0 and yp==1: fp+=1\n",
    "        else: fn+=1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return round((1+beta**2)*precision*recall/(beta**2*precision+recall), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
      "y_pred = np.array([1, 0, 1, 0, 0, 1])\n",
      "beta = 1\n",
      "print(f_score(y_true, y_pred, beta))\n",
      "\n",
      "Output:\n",
      "0.857\n",
      "\n",
      "Expected:\n",
      "0.857\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "y_true = np.array([1, 0, 1, 1, 0, 0])\n",
      "y_pred = np.array([1, 0, 0, 0, 0, 1])\n",
      "beta = 1\n",
      "print(f_score(y_true, y_pred, beta))\n",
      "\n",
      "Output:\n",
      "0.4\n",
      "\n",
      "Expected:\n",
      "0.4\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "y_true = np.array([1, 0, 1, 1, 0, 0])\n",
      "y_pred = np.array([1, 0, 1, 1, 0, 0])\n",
      "beta = 2\n",
      "print(f_score(y_true, y_pred, beta))\n",
      "\n",
      "Output:\n",
      "1.0\n",
      "\n",
      "Expected:\n",
      "1.0\n",
      "\n",
      "\n",
      "Test Case 4: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
      "y_pred = np.array([0, 0, 0, 1, 0, 1])\n",
      "beta = 2\n",
      "print(f_score(y_true, y_pred, beta))\n",
      "\n",
      "Output:\n",
      "0.556\n",
      "\n",
      "Expected:\n",
      "0.556\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1])\n",
    "beta = 1\n",
    "fs = f_score(y_true, y_pred, beta)\n",
    "print('Test Case 1: Accepted') if fs == 0.857 else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ny_true = np.array([1, 0, 1, 1, 0, 1])\\ny_pred = np.array([1, 0, 1, 0, 0, 1])\\nbeta = 1\\nprint(f_score(y_true, y_pred, beta))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(fs)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.857')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 0, 1])\n",
    "beta = 1\n",
    "fs = f_score(y_true, y_pred, beta)\n",
    "print('Test Case 2: Accepted') if fs == 0.4 else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ny_true = np.array([1, 0, 1, 1, 0, 0])\\ny_pred = np.array([1, 0, 0, 0, 0, 1])\\nbeta = 1\\nprint(f_score(y_true, y_pred, beta))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(fs)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.4')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0])\n",
    "y_pred = np.array([1, 0, 1, 1, 0, 0])\n",
    "beta = 2\n",
    "fs = f_score(y_true, y_pred, beta)\n",
    "print('Test Case 3: Accepted') if fs == 1.0 else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ny_true = np.array([1, 0, 1, 1, 0, 0])\\ny_pred = np.array([1, 0, 1, 1, 0, 0])\\nbeta = 2\\nprint(f_score(y_true, y_pred, beta))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(fs)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('1.0')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 1, 0, 1])\n",
    "beta = 2\n",
    "fs = f_score(y_true, y_pred, beta)\n",
    "print('Test Case 4: Accepted') if fs == 0.556 else print('Test Case 4: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ny_true = np.array([1, 0, 1, 1, 0, 1])\\ny_pred = np.array([0, 0, 0, 1, 0, 1])\\nbeta = 2\\nprint(f_score(y_true, y_pred, beta))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(fs)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.556')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
