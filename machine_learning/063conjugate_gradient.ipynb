{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Conjugate Gradient Method for Solving Linear Systems\n",
    "\n",
    "## Task: Implement the Conjugate Gradient Method for Solving Linear Systems\n",
    "\n",
    "Your task is to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. Given a matrix A and a vector b, the algorithm will solve for x in the system Ax = b.\n",
    "\n",
    "Write a function conjugate_gradient(A, b, n, x0=None, tol=1e-8) that performs the Conjugate Gradient method as follows:\n",
    "\n",
    "- A: A symmetric, positive-definite matrix representing the linear system.\n",
    "- b: The vector on the right side of the equation.\n",
    "- n: Maximum number of iterations.\n",
    "- x0: Initial guess for the solution vector.\n",
    "- tol: Tolerance for stopping criteria.\n",
    "\n",
    "The function should return the solution vector x.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 1], [1, 3]])\n",
    "b = np.array([1, 2])\n",
    "n = 5\n",
    "\n",
    "print(conjugate_gradient(A, b, n))\n",
    "\n",
    "# Expected Output:\n",
    "# [0.09090909, 0.63636364]\n",
    "```\n",
    "\n",
    "## Understanding The Conjugate Gradient Method\n",
    "\n",
    "The Conjugate Gradient, CG, method is an iterative algorithm used to solve large systems of linear equations, particularly those that are symmetric and positive-definite.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "The CG gradient method is often applied to the quadratic form of a linear system, Ax = b:\n",
    " \n",
    "$$f(x) = \\frac{1}{2}x^TAx - b^Tx$$\n",
    " \n",
    "The quadratic form is used due to its' differential reducing to, the following for a symmetric A. Therefore, x satisfies Ax = b, at the optimum:\n",
    "\n",
    "$$\\nabla f(x) = Ax - b = 0$$\n",
    "\n",
    "The conjugate gradient method uses search directions that are conjugate to all the previous search direction. This is satisfied when search directions are A-orthogonal, i.e.\n",
    "\n",
    "$$p_i^TAp_j = 0, \\forall i \\neq j$$\n",
    "\n",
    "This results in a more efficient algorithm, as it ensures that the algorithm gathers all information in a search direction at once, and then doesn't need to search in that direction again. As opposed to steepest descent, that will step a bit in one direction and then may search in that direction later.\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "1. Initialization:\n",
    "- $x_0$: Initial guess for the variable vector.\n",
    "- $r_0=b-Ax_0$: Initial residual vector.\n",
    "- $p_0=r_0$: Initial search direction.\n",
    "2. Iteration k:\n",
    "- $\\alpha_k = \\frac{r_k^Tr_k}{p_k^TAp_k}$: Step size.\n",
    "- $x_{k+1} = x_k + \\alpha_kp_k$: Update solution.\n",
    "- $r_{k+1} = r_k - \\alpha_kAp_k$: Update residual.\n",
    "- Check convergence: $||r_{k+1}|| < \\text{tolerance}$.\n",
    "- $\\beta_k = \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}$: New direction scaling. This ensures search directions are A-orthogonal.\n",
    "- $p_{k+1} = r_{k+1} + \\beta_kp_k$: Update search direction.\n",
    "3. Termination:\n",
    "- Stop when $||r_{k+1}|| < \\text{tolerance}$ or after a set number of iterations.\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "Let's solve the system of equations:\n",
    "\n",
    "$$4x_1 + x_2 = 6, x_1 + 3x_2 = 6$$\n",
    "\n",
    "1. Initialize $x_0 = [0, 0]^T$, $r_0 = b - Ax_0 = [6, 6]^T$, $p_0 = r_0 = [6, 6]^T$.\n",
    "2. First iteration:\n",
    "- Compute $\\alpha_0$:\n",
    "  $$\\alpha_0 = \\frac{r_0^Tr_0}{p_0^TAp_0} = \\frac{72}{324} = 0.2222$$\n",
    "- Update solution $x_1$:\n",
    "  $$x_1 = x_0 + \\alpha_0p_0 = [0, 0]^T + 0.2222[6, 6]^T = [1.3333, 1.3333]^T$$\n",
    "- Update residual $r_1$:\n",
    "  $$r_1 = r_0 - \\alpha_0Ap_0 = [6, 6]^T - 0.2222 \\cdot \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\cdot [6, 6]^T = [6.67, 5.33]^T$$\n",
    "- Compute $\\beta_0$:\n",
    "  $$\\beta_0 = \\frac{r_1^Tr_1}{r_0^Tr_0} = \\frac{6.67^2 + 5.33^2}{6^2 + 6^2} \\approx 0.99$$\n",
    "- Update search direction $p_1$:\n",
    "  $$p_1 = r_1 + \\beta_0p_0 = [6.67, 5.33]^T + 0.99[6, 6]^T = [12.60, 11.26]^T$$\n",
    "3. Second iteration:\n",
    "- Compute $\\alpha_1$, $x_2$, $r_2$, and repeat the process until convergence.\n",
    "\n",
    "## Applications\n",
    "\n",
    "The conjugate gradient method is often used as it's more efficient that other iterative solvers, such as steepest descent, and direct solvers, such as Gaussian Elimination. Iterative linear solvers are commonly used in optimisation, machine learning and computational fluid dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8) -> np.array:\n",
    "    \"\"\"\n",
    "    Solve the system Ax = b using the Conjugate Gradient method.\n",
    "\n",
    "    :param A: Symmetric positive-definite matrix\n",
    "    :param b: Right-hand side vector\n",
    "    :param n: Maximum number of iterations\n",
    "    :param x0: Initial guess for solution (default is zero vector)\n",
    "    :param tol: Convergence tolerance\n",
    "    :return: Solution vector x\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate initial residual vector\n",
    "    x = np.zeros_like(b)\n",
    "    r = residual(A, b, x) # residual vector\n",
    "    rPlus1 = r\n",
    "    p = r # search direction vector\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        # line search step value - this minimizes the error along the current search direction\n",
    "        alp = alpha(A, r, p)\n",
    "\n",
    "        # new x and r based on current p (the search direction vector)\n",
    "        x = x + alp * p\n",
    "        rPlus1 = r - alp * (A @ p)\n",
    "\n",
    "        # calculate beta - this ensures that all vectors are A-orthogonal to each other\n",
    "        bet = beta(r, rPlus1)\n",
    "\n",
    "        # update x and r\n",
    "        # using a othogonal search direction ensures we get all the information we need in more direction and then don't have to search in that direction again\n",
    "        p = rPlus1 + bet * p\n",
    "\n",
    "        # update residual vector\n",
    "        r = rPlus1\n",
    "\n",
    "        # break if less than tolerance\n",
    "        if np.linalg.norm(residual(A, b, x)) < tol:\n",
    "            break\n",
    "\n",
    "    return x\n",
    "\n",
    "def residual(A: np.array, b: np.array, x: np.array) -> np.array:\n",
    "    # calculate linear system residuals\n",
    "    return b - A @ x\n",
    "\n",
    "def alpha(A: np.array, r: np.array, p: np.array) -> float:\n",
    "\n",
    "    # calculate step size\n",
    "    alpha_num = np.dot(r, r)\n",
    "    alpha_den = np.dot(p @ A, p)\n",
    "\n",
    "    return alpha_num/alpha_den\n",
    "\n",
    "def beta(r: np.array, r_plus1: np.array) -> float:\n",
    "\n",
    "    # calculate direction scaling\n",
    "    beta_num = np.dot(r_plus1, r_plus1)\n",
    "    beta_den = np.dot(r, r)\n",
    "\n",
    "    return beta_num/beta_den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "A = np.array([[4, 1], [1, 3]])\n",
      "b = np.array([1, 2])\n",
      "n = 5\n",
      "print(conjugate_gradient(A, b, n))\n",
      "\n",
      "Output:\n",
      "[0.09090909 0.63636364]\n",
      "\n",
      "Expected:\n",
      "[0.09090909, 0.63636364]\n",
      "\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "A = np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]])\n",
      "b = np.array([7, 8, 5])\n",
      "n = 1\n",
      "print(conjugate_gradient(A, b, n))\n",
      "\n",
      "Output:\n",
      "[1.2627451  1.44313725 0.90196078]\n",
      "\n",
      "Expected:\n",
      "[1.2627451, 1.44313725, 0.90196078]\n",
      "\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "A = np.array([[6, 2, 1, 1, 0],\n",
      "              [2, 5, 2, 1, 1],\n",
      "              [1, 2, 6, 1, 2],\n",
      "              [1, 1, 1, 7, 1],\n",
      "              [0, 1, 2, 1, 8]])\n",
      "b = np.array([1, 2, 3, 4, 5])\n",
      "n = 100\n",
      "print(conjugate_gradient(A, b, n))\n",
      "\n",
      "Output:\n",
      "[0.01666667 0.11666667 0.21666667 0.45       0.5       ]\n",
      "\n",
      "Expected:\n",
      "[0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[4, 1], [1, 3]])\n",
    "b = np.array([1, 2])\n",
    "n = 5\n",
    "output = conjugate_gradient(A, b, n)\n",
    "print('Test Case 1: Accepted') if np.allclose(output, [0.09090909, 0.63636364]) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nA = np.array([[4, 1], [1, 3]])\\nb = np.array([1, 2])\\nn = 5\\nprint(conjugate_gradient(A, b, n))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[0.09090909, 0.63636364]')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "A = np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]])\n",
    "b = np.array([7, 8, 5])\n",
    "n = 1\n",
    "output = conjugate_gradient(A, b, n)\n",
    "print('Test Case 2: Accepted') if np.allclose(output, [1.2627451, 1.44313725, 0.90196078]) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nA = np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]])\\nb = np.array([7, 8, 5])\\nn = 1\\nprint(conjugate_gradient(A, b, n))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[1.2627451, 1.44313725, 0.90196078]')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "A = np.array([[6, 2, 1, 1, 0],\n",
    "              [2, 5, 2, 1, 1],\n",
    "              [1, 2, 6, 1, 2],\n",
    "              [1, 1, 1, 7, 1],\n",
    "              [0, 1, 2, 1, 8]])\n",
    "b = np.array([1, 2, 3, 4, 5])\n",
    "n = 100\n",
    "output = conjugate_gradient(A, b, n)\n",
    "print('Test Case 3: Accepted') if np.allclose(output, [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]) else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nA = np.array([[6, 2, 1, 1, 0],\\n              [2, 5, 2, 1, 1],\\n              [1, 2, 6, 1, 2],\\n              [1, 1, 1, 7, 1],\\n              [0, 1, 2, 1, 8]])\\nb = np.array([1, 2, 3, 4, 5])\\nn = 100\\nprint(conjugate_gradient(A, b, n))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
