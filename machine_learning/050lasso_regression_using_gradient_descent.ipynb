{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Lasso Regression using Gradient Descent\n",
    "\n",
    "In this problem, you need to implement the Lasso Regression algorithm using Gradient Descent. Lasso Regression (L1 Regularization) adds a penalty equal to the absolute value of the coefficients to the loss function. Your task is to update the weights and bias iteratively using the gradient of the loss function and the L1 penalty.\n",
    "\n",
    "The objective function of Lasso Regression is:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{n} \\sum_{i=1}^{2n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$ \n",
    " \n",
    "Where:\n",
    "\n",
    "- $y_i$ is the actual value for the $i$-th sample\n",
    "- $\\hat{y}_i = \\sum_{j=1}^{p} X_{ij} w_j + b$ is the predicted value for the $i$-th sample\n",
    "- $w_j$ is the weight associated with the $j$-th feature\n",
    "- $\\alpha$ is the regularization parameter\n",
    "- $b$ is the bias\n",
    " \n",
    "Your task is to use the L1 penalty to shrink some of the feature coefficients to zero during gradient descent, thereby helping with feature selection.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [1, 1], [2, 2]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "alpha = 0.1\n",
    "weights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
    "\n",
    "# Expected Output:\n",
    "(weights,bias)\n",
    "(array([float, float]), float)\n",
    "```\n",
    "  \n",
    "## Understanding Lasso Regression and L1 Regularization\n",
    "\n",
    "Lasso Regression is a type of linear regression that applies L1 regularization to the model. It adds a penalty equal to the sum of the absolute values of the coefficients, encouraging some of them to be exactly zero. This makes Lasso Regression particularly useful for feature selection, as it can shrink the coefficients of less important features to zero, effectively removing them from the model.\n",
    "\n",
    "## Steps to Implement Lasso Regression using Gradient Descent\n",
    "\n",
    "- Initialize Weights and Bias: Start with the weights and bias set to zero.\n",
    "- Make Predictions: Use the formula:\n",
    "\n",
    "$$\\hat{y}_i = \\sum_{j=1}^{p} X_{ij} w_j + b$$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted value for the $i$-th sample.\n",
    "\n",
    "- Compute Residuals: Find the difference between the actual values $y_i$ and the predicted values $\\hat{y}_i$. These residuals are the errors in the model.\n",
    "- Update the Weights and Bias: Update the weights and bias using the gradient of the loss function with respect to the weights and bias:\n",
    "\n",
    "    - For weights $w_j$:\n",
    "    \n",
    "    $$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij} (y_i - \\hat{y}_i) + \\alpha \\cdot \\text{sign}(w_j)$$\n",
    " \n",
    "    - For bias $b$ (without the regularization term):\n",
    "\n",
    "    $$\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n",
    "    \n",
    "    - Update the weights and bias:\n",
    " \n",
    "    $$w_j = w_j - \\eta \\cdot \\frac{\\partial J}{\\partial w_j}$$\n",
    "\n",
    "    $$b = b - \\eta \\cdot \\frac{\\partial J}{\\partial b}$$\n",
    " \n",
    "- Check for Convergence: The algorithm stops when the L1 norm of the gradient with respect to the weights becomes smaller than a predefined threshold $tol$:\n",
    "\n",
    "$$\\| \\nabla_w J \\|_1 = \\sum_{j=1}^{p} |\\frac{\\partial J}{\\partial w_j}| < tol$$ \n",
    " \n",
    "Return the Weights and Bias: Once the algorithm converges, return the optimized weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n",
    "    n_samples, n_features = X.shape\n",
    "    # Zero out weights and bias\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Predict values\n",
    "        y_pred = np.dot(X, weights) + bias\n",
    "        # Calculate error\n",
    "        error = y_pred - y\n",
    "        # Gradient for weights with L1 penalty\n",
    "        grad_w = (1 / n_samples) * np.dot(X.T, error) + alpha * np.sign(weights)\n",
    "        # Gradient for bias (no penalty for bias)\n",
    "        grad_b = (1 / n_samples) * np.sum(error)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * grad_w\n",
    "        bias -= learning_rate * grad_b\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad_w, ord=1) < tol:\n",
    "            break\n",
    "    \n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[0, 0], [1, 1], [2, 2]])\n",
      "y = np.array([0, 1, 2])\n",
      "alpha = 0.1\n",
      "output = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "(array([0.42371644, 0.42371644]), 0.15385068459377865)\n",
      "\n",
      "Expected:\n",
      "(array([0.42371644, 0.42371644]), 0.15385068459377865)\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\n",
      "y = np.array([1, 2, 3, 4, 5])\n",
      "alpha = 0.1\n",
      "output = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "(array([0.27280148, 0.68108784]), 0.40828636087180054)\n",
      "\n",
      "Expected:\n",
      "(array([0.27280148, 0.68108784]), 0.4082863608718005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0], [1, 1], [2, 2]])\n",
    "y = np.array([0, 1, 2])\n",
    "alpha = 0.1\n",
    "output = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
    "print('Test Case 1: Accepted') if np.allclose(output[0], np.array([0.42371644, 0.42371644])) and np.isclose(output[1], 0.15385068459377865) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[0, 0], [1, 1], [2, 2]])\\ny = np.array([0, 1, 2])\\nalpha = 0.1\\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('(array([0.42371644, 0.42371644]), 0.15385068459377865)')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "alpha = 0.1\n",
    "output = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n",
    "print('Test Case 2: Accepted') if np.allclose(output[0], np.array([0.27280148, 0.68108784])) and np.isclose(output[1], 0.4082863608718005) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\\ny = np.array([1, 2, 3, 4, 5])\\nalpha = 0.1\\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('(array([0.27280148, 0.68108784]), 0.4082863608718005)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
