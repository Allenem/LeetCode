{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Adam Optimization Algorithm\n",
    "Implement the Adam (Adaptive Moment Estimation) optimization algorithm in Python. Adam is an optimization algorithm that adapts the learning rate for each parameter. Your task is to write a function `adam_optimizer` that updates the parameters of a given function using the Adam algorithm.\n",
    "\n",
    "The function should take the following parameters:\n",
    "\n",
    "- `f`: The objective function to be optimized\n",
    "- `grad`: A function that computes the gradient of `f`\n",
    "- `x0`: Initial parameter values\n",
    "- `learning_rate`: The step size (default: 0.001)\n",
    "- `beta1`: Exponential decay rate for the first moment estimates (default: 0.9)\n",
    "- `beta2`: Exponential decay rate for the second moment estimates (default: 0.999)\n",
    "- `epsilon`: A small constant for numerical stability (default: 1e-8)\n",
    "- `num_iterations`: Number of iterations to run the optimizer (default: 1000)\n",
    "\n",
    "The function should return the optimized parameters.\n",
    "\n",
    "Example\n",
    "```py\n",
    "import numpy as np\n",
    "\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "x0 = np.array([1.0, 1.0])\n",
    "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
    "\n",
    "print(\"Optimized parameters:\", x_opt)\n",
    "\n",
    "# Expected Output:\n",
    "# Optimized parameters: [0.99000325 0.99000325]\n",
    "```\n",
    "\n",
    "## Understanding the Adam Optimization Algorithm\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm commonly used in training deep neural networks. It combines ideas from two other optimization algorithms: RMSprop and Momentum.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. `Adaptive Learning Rates`: Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "\n",
    "2. `Momentum`: It keeps track of an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    "3. `RMSprop`: It also keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "4. `Bias Correction`: Adam includes bias correction terms to account for the initialization of the first and second moment estimates.\n",
    "\n",
    "## The Adam Algorithm\n",
    "\n",
    "Given parameters $\\theta$, objective function $f(\\theta)$, and its gradient $\\nabla_\\theta f(\\theta)$:\n",
    "\n",
    "1. Initialize time step $t = 0$, parameters $\\theta$, first moment vector $m_0 = 0$, second moment vector $v_0 = 0$, and hyperparameters $\\alpha$ (learning rate), $\\beta_1$, $\\beta_2$, and $\\epsilon$.\n",
    "\n",
    "2. While not converged, do:\n",
    "    1. Increment time step: $t = t + 1$\n",
    "    2. Compute gradient: $g_t = \\nabla_\\theta f_t(\\theta_{t-1})$\n",
    "    3. Update biased first moment estimate: $m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$\n",
    "    4. Update biased second raw moment estimate: $v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$\n",
    "    5. Compute bias-corrected first moment estimate: $\\hat{m}_t = m_t / (1 - \\beta_1^t)$\n",
    "    6. Compute bias-corrected second raw moment estimate: $\\hat{v}_t = v_t / (1 - \\beta_2^t)$\n",
    "    7. Update parameters: $\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\epsilon)$\n",
    "    \n",
    "Adam combines the advantages of AdaGrad, which works well with sparse gradients, and RMSProp, which works well in online and non-stationary settings. Adam is generally regarded as being fairly robust to the choice of hyperparameters, though the learning rate may sometimes need to be changed from the suggested default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n",
    "    x = x0\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    for i in range(num_iterations):\n",
    "        g = grad(x)\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * g**2\n",
    "        m_hat = m / (1 - beta1**(i+1))\n",
    "        v_hat = v / (1 - beta2**(i+1))\n",
    "        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "def objective_function(x):\n",
      "    return x[0]**2 + x[1]**2\n",
      "def gradient(x):\n",
      "    return np.array([2*x[0], 2*x[1]])\n",
      "x0 = np.array([1.0, 1.0])\n",
      "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
      "print(x_opt)\n",
      "\n",
      "Output:\n",
      "[0.99000325 0.99000325]\n",
      "\n",
      "Expected:\n",
      "[0.99000325 0.99000325]\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "def objective_function(x):\n",
      "    return x[0]**2 + x[1]**2\n",
      "def gradient(x):\n",
      "    return np.array([2*x[0], 2*x[1]])\n",
      "x0 = np.array([0.2, 12.3])\n",
      "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
      "print(x_opt)\n",
      "\n",
      "Output:\n",
      "[ 0.19001678 12.29000026]\n",
      "\n",
      "Expected:\n",
      "[ 0.19001678 12.29000026]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "def gradient(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "x0 = np.array([1.0, 1.0])\n",
    "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
    "print('Test Case 1: Accepted') if np.allclose(x_opt, [0.99000325, 0.99000325]) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ndef objective_function(x):\\n    return x[0]**2 + x[1]**2\\ndef gradient(x):\\n    return np.array([2*x[0], 2*x[1]])\\nx0 = np.array([1.0, 1.0])\\nx_opt = adam_optimizer(objective_function, gradient, x0)\\nprint(x_opt)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(x_opt)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[0.99000325 0.99000325]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "def gradient(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "x0 = np.array([0.2, 12.3])\n",
    "x_opt = adam_optimizer(objective_function, gradient, x0)\n",
    "print('Test Case 2: Accepted') if np.allclose(x_opt, [0.19001678, 12.29000026]) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\ndef objective_function(x):\\n    return x[0]**2 + x[1]**2\\ndef gradient(x):\\n    return np.array([2*x[0], 2*x[1]])\\nx0 = np.array([0.2, 12.3])\\nx_opt = adam_optimizer(objective_function, gradient, x0)\\nprint(x_opt)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(x_opt)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[ 0.19001678 12.29000026]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
