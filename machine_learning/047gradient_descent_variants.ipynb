{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Gradient Descent Variants with MSE Loss\n",
    "In this problem, you need to implement a single function that can perform three variants of gradient descent—Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent—using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(X.shape[1])\n",
    "\n",
    "# Test Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
    "output: [float,float]\n",
    "# Test Stochastic Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
    "output: [float, float]\n",
    "# Test Mini-Batch Gradient Descent\n",
    "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
    "output: [float, float]\n",
    "```\n",
    "\n",
    "## Understanding Gradient Descent Variants with MSE Loss\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, particularly in linear regression and neural networks. The Mean Squared Error (MSE) loss function is commonly used in regression tasks. There are three main types of gradient descent based on how much data is used to compute the gradient at each iteration:\n",
    "\n",
    "## 1. Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent computes the gradient of the MSE loss function with respect to the parameters for the entire training dataset. It updates the parameters after processing the entire dataset:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\frac{2}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, $m$ is the number of samples, and $\\nabla_\\theta J(\\theta)$ is the gradient of the MSE loss function.\n",
    "\n",
    "## 2. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent updates the parameters for each training example individually, making it faster but more noisy:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot 2(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    "\n",
    "Where $x^{(i)}$ and $y^{(i)}$ are individual training examples.\n",
    "\n",
    "## 3. Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-Batch Gradient Descent is a compromise between Batch and Stochastic Gradient Descent. It updates the parameters after processing a small batch of training examples, without shuffling the data:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\frac{2}{b} \\sum_{i = 1}^b (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$ \n",
    " \n",
    "Where $b$ is the batch size, a subset of the training dataset.\n",
    "\n",
    "Each method has its advantages: Batch Gradient Descent is more stable but slower, Stochastic Gradient Descent is faster but noisy, and Mini-Batch Gradient Descent strikes a balance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
    "    m = len(y)\n",
    "    for _ in range(n_iterations):\n",
    "        if method=='batch':\n",
    "            y_ = X.dot(weights)\n",
    "            errs = y_ - y\n",
    "            gradient = 2 * X.T.dot(errs) / m\n",
    "            weights = weights - learning_rate * gradient\n",
    "        elif method=='stochastic':\n",
    "            for i in range(m):\n",
    "                yi_ = X[i].dot(weights)\n",
    "                err = yi_ - y[i]\n",
    "                gradient = 2 * X[i].T.dot(err)\n",
    "                weights = weights - learning_rate * gradient\n",
    "        elif method=='mini_batch':\n",
    "            for i in range(0, m, batch_size):\n",
    "                xbi = X[i:i+batch_size]\n",
    "                ybi = y[i:i+batch_size]\n",
    "                ybi_ = xbi.dot(weights)\n",
    "                errs = ybi_ - ybi\n",
    "                gradient = 2 * xbi.T.dot(errs) / batch_size\n",
    "                weights = weights - learning_rate * gradient\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
      "y = np.array([2, 3, 4, 5])\n",
      "weights = np.zeros(X.shape[1])\n",
      "learning_rate = 0.01\n",
      "n_iterations = 100\n",
      "# Test Batch Gradient Descent\n",
      "output = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[1.14905239 0.56176776]\n",
      "\n",
      "Expected:\n",
      "[1.14905239 0.56176776]\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
      "y = np.array([2, 3, 4, 5])\n",
      "weights = np.zeros(X.shape[1])\n",
      "learning_rate = 0.01\n",
      "n_iterations = 100\n",
      "# Test Stochastic Gradient Descent\n",
      "output = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[1.0507814  0.83659454]\n",
      "\n",
      "Expected:\n",
      "[1.0507814  0.83659454]\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
      "y = np.array([2, 3, 4, 5])\n",
      "weights = np.zeros(X.shape[1])\n",
      "learning_rate = 0.01\n",
      "n_iterations = 100\n",
      "batch_size = 2\n",
      "# Test Mini-Batch Gradient Descent\n",
      "output = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
      "print(output)\n",
      "\n",
      "Output:\n",
      "[1.10334065 0.68329431]\n",
      "\n",
      "Expected:\n",
      "[1.10334065 0.68329431]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "weights = np.zeros(X.shape[1])\n",
    "learning_rate = 0.01\n",
    "n_iterations = 100\n",
    "# Test Batch Gradient Descent\n",
    "output = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
    "print('Test Case 1: Accepted') if np.allclose(output, [1.14905239, 0.56176776]) else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\\ny = np.array([2, 3, 4, 5])\\nweights = np.zeros(X.shape[1])\\nlearning_rate = 0.01\\nn_iterations = 100\\n# Test Batch Gradient Descent\\noutput = gradient_descent(X, y, weights, learning_rate, n_iterations, method=\\'batch\\')\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[1.14905239 0.56176776]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "weights = np.zeros(X.shape[1])\n",
    "learning_rate = 0.01\n",
    "n_iterations = 100\n",
    "# Test Stochastic Gradient Descent\n",
    "output = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
    "print('Test Case 2: Accepted') if np.allclose(output, [1.0507814, 0.83659454]) else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\\ny = np.array([2, 3, 4, 5])\\nweights = np.zeros(X.shape[1])\\nlearning_rate = 0.01\\nn_iterations = 100\\n# Test Stochastic Gradient Descent\\noutput = gradient_descent(X, y, weights, learning_rate, n_iterations, method=\\'stochastic\\')\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[1.0507814  0.83659454]')\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "weights = np.zeros(X.shape[1])\n",
    "learning_rate = 0.01\n",
    "n_iterations = 100\n",
    "batch_size = 2\n",
    "# Test Mini-Batch Gradient Descent\n",
    "output = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
    "print('Test Case 3: Accepted') if np.allclose(output, [1.10334065, 0.68329431]) else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\\ny = np.array([2, 3, 4, 5])\\nweights = np.zeros(X.shape[1])\\nlearning_rate = 0.01\\nn_iterations = 100\\nbatch_size = 2\\n# Test Mini-Batch Gradient Descent\\noutput = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method=\\'mini_batch\\')\\nprint(output)')\n",
    "print()\n",
    "print('Output:')\n",
    "print(output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('[1.10334065 0.68329431]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
