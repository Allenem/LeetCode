{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence Between Two Normal Distributions\n",
    "\n",
    "Task: Implement KL Divergence Between Two Normal Distributions\n",
    "Your task is to compute the Kullback-Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n",
    "\n",
    "Write a function `kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)` that calculates the KL divergence between two normal distributions, where $P \\sim N(\\mu_p, \\sigma_p^2)$ and $Q \\sim N(\\mu_q, \\sigma_q^2)$.\n",
    "\n",
    "The function should return the KL divergence as a floating-point number.\n",
    "\n",
    "Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "mu_p = 0.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 1.0\n",
    "sigma_q = 1.0\n",
    "\n",
    "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
    "\n",
    "# Expected Output:\n",
    "# 0.5\n",
    "```\n",
    "\n",
    "## Understanding Kullback-Leibler Divergence (KL Divergence)\n",
    "\n",
    "The Kullback-Leibler divergence, also known as relative entropy, is a statistical tool that measures the difference between two probability distributions.\n",
    "\n",
    "## Definition of KL Divergence\n",
    "\n",
    "For continuous variables, the KL divergence is defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right) dx\n",
    "$$\n",
    " \n",
    "## KL Divergence Between Two Normal Distributions\n",
    "\n",
    "Suppose we have two normal distributions, $P$ and $Q$, where:\n",
    "\n",
    "$$\n",
    "P \\sim N(\\mu_p, \\sigma_p^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q \\sim N(\\mu_q, \\sigma_q^2)\n",
    "$$\n",
    "\n",
    "The KL divergence between $P$ and $Q$ is given by:\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\log \\left( \\frac{\\sigma_q}{\\sigma_p} \\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2 \\sigma_q^2} - \\frac{1}{2}\n",
    "$$\n",
    " \n",
    "## Interpretation\n",
    "\n",
    "This formula quantifies how one normal distribution $P$ diverges from another normal distribution $Q$. A KL divergence of zero indicates that the two distributions are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    return np.log(sigma_q/sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2*sigma_q**2) - .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "mu_p = 0.0\n",
      "sigma_p = 1.0\n",
      "mu_q = 0.0\n",
      "sigma_q = 1.0\n",
      "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
      "\n",
      "Output:\n",
      "0.0\n",
      "\n",
      "Expected:\n",
      "0.0\n",
      "\n",
      "\n",
      "Test Case 2: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "mu_p = 0.0\n",
      "sigma_p = 1.0\n",
      "mu_q = 1.0\n",
      "sigma_q = 1.0\n",
      "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
      "\n",
      "Output:\n",
      "0.5\n",
      "\n",
      "Expected:\n",
      "0.5\n",
      "\n",
      "\n",
      "Test Case 3: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "mu_p = 0.0\n",
      "sigma_p = 1.0\n",
      "mu_q = 0.0\n",
      "sigma_q = 2.0\n",
      "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
      "\n",
      "Output:\n",
      "0.3181471805599453\n",
      "\n",
      "Expected:\n",
      "0.3181471805599453\n",
      "\n",
      "\n",
      "Test Case 4: Accepted\n",
      "Input:\n",
      "import numpy as np\n",
      "mu_p = 1.0\n",
      "sigma_p = 1.0\n",
      "mu_q = 0.0\n",
      "sigma_q = 2.0\n",
      "print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n",
      "\n",
      "Output:\n",
      "0.4431471805599453\n",
      "\n",
      "Expected:\n",
      "0.4431471805599453\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mu_p = 0.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 0.0\n",
    "sigma_q = 1.0\n",
    "Output = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n",
    "print('Test Case 1: Accepted') if Output == 0.0 else print('Test Case 1: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nmu_p = 0.0\\nsigma_p = 1.0\\nmu_q = 0.0\\nsigma_q = 1.0\\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(Output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.0')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "mu_p = 0.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 1.0\n",
    "sigma_q = 1.0\n",
    "Output = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n",
    "print('Test Case 2: Accepted') if Output == 0.5 else print('Test Case 2: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nmu_p = 0.0\\nsigma_p = 1.0\\nmu_q = 1.0\\nsigma_q = 1.0\\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(Output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.5')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "mu_p = 0.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 0.0\n",
    "sigma_q = 2.0\n",
    "Output = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n",
    "print('Test Case 3: Accepted') if Output == 0.3181471805599453 else print('Test Case 3: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nmu_p = 0.0\\nsigma_p = 1.0\\nmu_q = 0.0\\nsigma_q = 2.0\\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(Output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.3181471805599453')\n",
    "print()\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "mu_p = 1.0\n",
    "sigma_p = 1.0\n",
    "mu_q = 0.0\n",
    "sigma_q = 2.0\n",
    "Output = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n",
    "print('Test Case 4: Accepted') if Output == 0.4431471805599453 else print('Test Case 4: Failed')\n",
    "print('Input:')\n",
    "print('import numpy as np\\nmu_p = 1.0\\nsigma_p = 1.0\\nmu_q = 0.0\\nsigma_q = 2.0\\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))')\n",
    "print()\n",
    "print('Output:')\n",
    "print(Output)\n",
    "print()\n",
    "print('Expected:')\n",
    "print('0.4431471805599453')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
